## Sprachmodelle informieren BIRD

- Man findet sehr viele Blogposts und Beschreibungen
- Hugging face Library, ich muss mich damit einlesen
- Tokanization, BIRD
- Viel mit D3 arbeiten, aber waere auch mit Python okay


Man versucht herauszufinden was BERT in den embeddings encodiert.

Was passiert mit de embedding Vektoren

### Idee von dem Projekt

- Transformermodelle, BIRD, Informationen werden ueber 3 Saetze
- Jedes Wort wird durch einen Vektor repraesentiert


- Was passiert wenn ein Wort nicht in dem dictionary drinnen ist sondern
es ist aufgeteilt 

- Was lernt das Modell fuer die Subwords, & wenn man sie averaged
Welche Woerter sind aehnlich zu diesem


- Ist es gut Averaging 

Nearest Neighbor, weil explorativ, um herauszufinde was in dem Modell steckt

Wenn man averaged bringt man etwas durcheinander -> Produziert man 
einen neues Embedding

https://embeddings-explained.lingvis.io/

Subwords learning, genug Paper darbueber?

https://lmfingerprints.lingvis.io/

https://adapters.demo.lingvis.io/

-> In den Paper immer die Subwords averaged und nicht untersucht
-> 
